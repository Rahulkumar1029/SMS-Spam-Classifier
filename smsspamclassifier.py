# -*- coding: utf-8 -*-
"""smsSpamClassifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DKSdLXNDwwS-xn9b7uvVy3bhdxoA2ma5
"""

import pandas as pd
import numpy as np

df=pd.read_csv("/content/spam.csv", encoding='latin-1')

df.head(2)

df.shape

#data cleaning
df.info()

df=df.iloc[:,[0,1]]

df.shape



df.rename(columns={'v1':'target','v2':'text'},inplace=True)
df.head(1)

df['target'].value_counts()

from sklearn.preprocessing import LabelEncoder
encoder=LabelEncoder()
df['target']=encoder.fit_transform(df['target'])
df.head()

df.isnull().sum()

df.duplicated().sum()

df.drop_duplicates(keep='first',inplace=True)

df.duplicated().sum()

import nltk
nltk.download('punkt_tab')

import matplotlib.pyplot as plt
import seaborn as sns

plt.pie(df['target'].value_counts(),labels=['ham','spam'],autopct="%0.2f")
plt.show()

#data is imbalance
import nltk

nltk.download('punkt')

df['num_characters']=df['text'].apply(len)

#number of words
df['num_words']=df['text'].apply(lambda x:len(nltk.word_tokenize(x)))

df['num_sentances']=df['text'].apply(lambda x:len(nltk.sent_tokenize(x)))

df.head()

df.describe()

df[df['target']==0].describe()

plt.figure(figsize=(12,8))
sns.histplot(df[df['target']==0]['num_characters'])
sns.histplot(df[df['target']==1]['num_characters'],color='red')

plt.figure(figsize=(12,8))
sns.histplot(df[df['target']==0]['num_words'])
sns.histplot(df[df['target']==1]['num_words'],color='red')

sns.pairplot(df,hue='target')

# Select only numeric columns
numeric_df = df.select_dtypes(include=['number'])
sns.heatmap(numeric_df.corr(), annot=True)
plt.show()

df.info()

from nltk.corpus import stopwords
import string
from string import punctuation
import nltk
nltk.download("stopwords")
from nltk.stem import PorterStemmer

# create an object of PorterStemmer
ps = PorterStemmer()

#text data processing
def process_text(text):
  text=text.lower()
  text=nltk.word_tokenize(text)
  y=[]
  for i in text:
    if i.isalnum():
      y.append(i)
  text=y[:]
  y.clear()

  for i in text:
    if i not in stopwords.words('english') and i not in string.punctuation:
      y.append(i)

  text=y[:]
  y.clear()

  for i in text:
    y.append(ps.stem(i))

  return " ".join(y)

df['transformed_text']=df['text'].apply(process_text)

df.head()

from wordcloud import WordCloud
wc=WordCloud(width=800,height=800,min_font_size=10,background_color='white')

spam_wc=wc.generate(df[df['target']==1]['transformed_text'].str.cat(sep=" "))

plt.imshow(spam_wc)

notspam_wc=wc.generate(df[df['target']==0]['transformed_text'].str.cat(sep=" "))
plt.imshow(notspam_wc)
plt.show()

spam_words=[]
for word in df[df['target']==1]['transformed_text'].tolist():
  for word in word.split():
    spam_words.append(word)

from collections import Counter
sns.barplot(
    x=pd.DataFrame(Counter(spam_words).most_common(30))[0],
    y=pd.DataFrame(Counter(spam_words).most_common(30))[1]
)
plt.xticks(rotation='vertical')
plt.show()

#model building
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer
cv=CountVectorizer()
tfidf=TfidfVectorizer(max_features=3000)

X=tfidf.fit_transform(df['transformed_text']).toarray()

X.shape

y=df['target'].values

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=2)

from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB
from sklearn.metrics import accuracy_score,confusion_matrix,precision_score

gnb=GaussianNB()
mnb=MultinomialNB()
bnb=BernoulliNB()

gnb.fit(X_train,y_train)
y_pred1=gnb.predict(X_test)
print(accuracy_score(y_test,y_pred1))
print(confusion_matrix(y_test,y_pred1))
print(precision_score(y_test,y_pred1))

mnb.fit(X_train,y_train)
y_pred2=mnb.predict(X_test)
print(accuracy_score(y_test,y_pred2))
print(confusion_matrix(y_test,y_pred2))
print(precision_score(y_test,y_pred2))

bnb.fit(X_train,y_train)
y_pred3=bnb.predict(X_test)
print(accuracy_score(y_test,y_pred3))
print(confusion_matrix(y_test,y_pred3))
print(precision_score(y_test,y_pred3))

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score
models = {
    "Logistic Regression": LogisticRegression(max_iter=2000),
    "Linear SVM": LinearSVC(),
    "Random Forest": RandomForestClassifier(n_estimators=200, random_state=42),
    "KNN": KNeighborsClassifier(n_neighbors=5),
    "mnb":MultinomialNB()
}
results = []
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred)
    results.append([name, acc, prec])

results_df = pd.DataFrame(results, columns=["Model", "Accuracy", "Precision"])
print(results_df)



